> ✅ -- 掌握理论知识，🖊 -- 需要手撕
# 🌱 基础知识 Checklist（简单）

-  Transformer 架构（Encoder / Decoder / Decoder-only） ✅
    
-  Self-Attention 公式（Q, K, V，缩放点积注意力） 🖊
    
-  Multi-Head Attention 的作用 ✅
    
-  Position Embedding（绝对 vs 相对） ✅
    
-  Causal LM vs MLM 的训练目标 ✅
    
-  交叉熵损失（Cross-Entropy Loss） 🖊
    
-  Teacher Forcing 与 Exposure Bias ✅
    
-  LayerNorm / RMSNorm 的区别 ✅
    
-  Adam / AdamW 原理 ✅
    
-  学习率调度（warmup, cosine decay） ✅
    
-  Greedy Search / Beam Search 🖊
    
-  Top-k / Top-p (Nucleus) Sampling 🖊
    

---

# 🌿 进阶知识 Checklist（中等）

-  Pretraining vs Finetuning ✅
    
-  Instruction Tuning（SFT） ✅
    
-  RLHF 流程（三阶段：SFT → RM → PPO） ✅
    
-  LoRA / QLoRA / Prefix Tuning ✅
    
-  数据并行、模型并行、流水线并行 ✅
    
-  ZeRO 优化（Stage 1/2/3） ✅
    
-  Checkpoint / Gradient Accumulation ✅
    
-  量化（Int8, Int4, GPTQ） ✅
    
-  蒸馏（Distillation） ✅
    
-  剪枝（Pruning） ✅
    
-  KV Cache 🖊
    
-  Speculative Decoding ✅
    
-  FlashAttention 🖊
    
-  Perplexity（困惑度计算公式） 🖊
    
-  BLEU, ROUGE, METEOR ✅
    
-  对话一致性、知识性、流畅性指标 ✅
    

---

# 🌳 高阶知识 Checklist（困难）

-  Rotary Position Embedding (RoPE) ✅
    
-  ALiBi ✅
    
-  MoE（Mixture of Experts） ✅
    
-  Scaling Laws（Chinchilla scaling law） ✅
    
-  Jailbreak / Prompt Injection 攻击 ✅
    
-  Red Teaming ✅
    
-  长上下文方法（Sparse Attention, Transformer-XL, RWKV, Mamba） ✅
    
-  多模态模型（CLIP, BLIP-2, LLaVA, Kosmos, Qwen-VL） ✅
    

---

# ⭐ 专项关注 Checklist（你重点需要）

### Agent

-  ReAct 框架（Reasoning + Acting） ✅
    
-  工具调用（Function Calling API） 🖊
    
-  记忆机制（短期记忆 vs 长期记忆） ✅
    
-  多 Agent 协作（AutoGen 等） ✅
    

### 应用

-  检索增强生成（RAG，Retriever + Generator） 🖊
    
-  医疗/金融场景下可靠性与可解释性 ✅
    
-  产品化中的延迟、成本、安全问题 ✅
    

### 多轮对话

-  对话状态追踪（DST） ✅
    
-  对话管理（Policy Learning） ✅
    
-  CoT（Chain-of-Thought）、ToT（Tree-of-Thought） ✅
    
-  用户意图漂移 / 上下文遗忘问题 ✅
    

### RL

-  RLHF 三阶段细节（SFT / RM / PPO） 🖊
    
-  DPO（Direct Preference Optimization） ✅
    
-  RLAIF（AI Feedback） ✅
    
-  Bandit 问题 vs MDP ✅
    
-  Exploration vs Exploitation ✅
    
-  Reward Hacking 问题 ✅
    

---

🖊 **常见手撕代码方向**（面试常考写伪代码或Python实现）：

- Self-Attention 前向计算
    
- Cross-Entropy Loss 实现
    
- Beam Search / Top-k / Top-p 采样
    
- KV Cache 实现
    
- FlashAttention 核心思路
    
- Perplexity 计算公式
    
