深度学习基础 
1. LN和BN的原理和区别 
2. 交叉熵的数学推导 
3. 交叉熵的代码手写 
4. sigmoid的代码手写 
5. 手撕多头注意力 
6. ReLU为什么能缓解梯度消失 
7. Adam优化器原理 
8. AUC计算方法 
9. Python装饰器作用 
10. KL散度 
11. softmax公式 
12. 梯度消失和梯度爆炸如何缓解 
13. 手撕NMS过程 
14. L1和L2正则的区别 
15. BN中可学习参数如何获取 
16. 如何缓解过拟合 
17. 介绍一下dropout 多模态/NLP算法 

# 多模态(偏大模型)
1. 介绍dpo算法原理 
2. gpt和bert的结构和参数量 
3. flash attention原理 
4. bert预训练任务，embedding 
5. fp16量化训练的策略 
6. qformer原理 
7. 了解哪些位置编码及原理 
8. clip原理 
9. blip2架构 
10. sft，lora和pretrain的区别 
11. llava和llama的区别 
12. 手撕BCE，InfoNCE损失 
13. 什么是大模型幻觉 
14. 混合精度训练是什么 
15. 很多大模型decoder-only原因 
16. 手撕RMSNorm 
17. deepspeed原理及使用 
18. peft微调介绍一下 
19. 介绍一下RAG 
