# hallucination 定义
* instances where a model confidently generates an answer that isn’t true. (cite: [Why language models hallucinate | OpenAI](https://openai.com/index/why-language-models-hallucinate/))
* plausible but false statements generated by language models. 似是而非的陈述。(cite: [Why language models hallucinate | OpenAI](https://openai.com/index/why-language-models-hallucinate/))


# hallucination 成因
> "language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty."

## Training
> hallucinations ***originate*** from Next-Word Prediction:
1. No labels to tell from valid statements (facts) to invalid ones. Models see stream of texts. 
2. For certain tasks, there is **no** learning patterns in data. *eg. Birthdays of crowds.*
	birthdays: arbitrary low-frequency facts without much pattern to learn. 
反例: 拼写、格式规范 是具有明显 pattern 的学习任务。
### Hallucination occurs. What type?




## Evaluation
Evaluation 评估本身并不会导致 hallucination 幻觉。**错误的奖励机制 incentive + 二元的判断标准** 导致了源于评估的幻觉。

例子：
现在的benchmarks 类似一场考试。
LLMs是考生。
对于不确定的题目，guessing 猜测有一定概率得分（选择题得分概率高达25%!). 但 acknowledging uncertainty 诚实表述不确定/ leave blank 则一定不得分。
这部分由于二元的 metric 设置 -- ACC，pass rate 导致的。

![[Pasted image 20250910021605.png]]
[^2]: 相比于gpt-4o, gpt-5 Acc 准确率较低，但犯错率显著低。更多是表达不确定。

采用猜测策略的模型会在现有leaderboards (especially acc-only scoreboards) 上得分超过 承认不确定性的 模型。

### A better evaluation schema?
**将confidence 纳入评估体系，并适当奖励 IDK 此类表示不确定性的回答。**
关键词： calibration evaluation, uncertainty evaluation. 


# Conclusion
Hallucinations can be eliminated by models acknowledging uncertainty regardless of the model size, not by managing 100% accuracy, which is impossible, for some realistic questions are not answerable. Current evaluations need to include confidence evaluations, and consider rewarding uncertainty. 